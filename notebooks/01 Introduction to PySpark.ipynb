{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b53c8e",
   "metadata": {},
   "source": [
    "# **Introduction to PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f60ea9",
   "metadata": {},
   "source": [
    "## Spark\n",
    "Spark is a platform that makes it easier to work with large datasets by spreading out the data and computations across **clusters**, containing multiple **nodes**. Each node is like a small computer, that can focus purely on a subset of the data. This way the total data can be computed in **parallel**\n",
    "\n",
    "### Spark in Python\n",
    "A spark cluster consists of one **master** node which controls multiple **worker** nodes. In practice the cluster is hosted on a remote machine (e.g. in Azure or DataBricks)\n",
    "\n",
    "Creating a connection to the spark cluster is done by using the `SparkContext` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c07528-6b9d-4e4a-a84f-ca3659627c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4eabcd97ff55:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "\n",
    "print(sc.version)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbdc42-1bfe-4887-aebb-07f071d78358",
   "metadata": {},
   "source": [
    "Once a connection with the cluster is established, the `SparkSession` class can be used to interface with the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9701a890-826d-4b63-a966-ec2190408a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4eabcd97ff55:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f06ecb42e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('example app').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68952136-d73f-4a82-a992-06f84b1860e2",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "The core data structure Spark uses is the **Resilient Distributed Dataset (RDD)**. This is a low level object that allows Spark to distribute data over its various nodes in the cluster. **DataFrames** are an abstraction built ontop of RDDs that make it easier to work with the data, for complex operations they can even be faster than RDDs.\n",
    "\n",
    "The simplest way to create a dataframe is to read it in from a csv file (similar to pandas). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d65ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+------------+-------------+\n",
      "|          name|continent|code|surface_area|geosize_group|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "|   Afghanistan|     Asia| AFG|      652090|       medium|\n",
      "|   Netherlands|   Europe| NLD|       41526|        small|\n",
      "|       Albania|   Europe| ALB|       28748|        small|\n",
      "|       Algeria|   Africa| DZA|     2381740|        large|\n",
      "|American Samoa|  Oceania| ASM|         199|        small|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/countries.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330a369-9303-4883-8d55-65c2335802dc",
   "metadata": {},
   "source": [
    "DataFrames contain metadata about their schema and paritions, which can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbcee5b-8e4d-4df0-9bf8-21eb0475eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Schema:\n",
      "-----------------\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- surface_area: string (nullable = true)\n",
      " |-- geosize_group: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('DataFrame Schema:')\n",
    "print('-----------------')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f1d3a0-5df5-4bfa-93cb-8ad28942f97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c31197",
   "metadata": {},
   "source": [
    "### Spark vs Pandas\n",
    "Spark DataFrames are similar to Pandas DataFrames, especially in their syntax. One can easily transform a Spark DataFrame into a Pandas DataFrame using `.toPandas()` and vice versa using `spark.createDataFrame()`\n",
    "\n",
    "Some key differences between a Pandas and Spark DataFrame are:\n",
    "\n",
    "* *Pandas*\n",
    "    * in-memory operations\n",
    "    * eager evaluation (operations are executed immediatly)\n",
    "    * small overhead\n",
    "    * optimized for small/medium datasets\n",
    "    * mutable\n",
    "    * integrates well with Python data-science eco-system (e.g. numpy, scikit-learn)\n",
    "\n",
    "* *Spark*\n",
    "    * distributed computing operations\n",
    "    * lazy evaluation (operations are evaluated when necessary)\n",
    "    * large overhead\n",
    "    * optimized for large datasets\n",
    "    * immutable\n",
    "    * integrates well with Apache Spark eco-system (e.g. MLlib, GraphX, Spark Streaming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b378df-cf54-454b-af54-0bb679b817f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>continent</th>\n",
       "      <th>code</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>geosize_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>AFG</td>\n",
       "      <td>652090</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NLD</td>\n",
       "      <td>41526</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Europe</td>\n",
       "      <td>ALB</td>\n",
       "      <td>28748</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Africa</td>\n",
       "      <td>DZA</td>\n",
       "      <td>2381740</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>ASM</td>\n",
       "      <td>199</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name continent code surface_area geosize_group\n",
       "0     Afghanistan      Asia  AFG       652090        medium\n",
       "1     Netherlands    Europe  NLD        41526         small\n",
       "2         Albania    Europe  ALB        28748         small\n",
       "3         Algeria    Africa  DZA      2381740         large\n",
       "4  American Samoa   Oceania  ASM          199         small"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.toPandas()\n",
    "df_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65aa81a-f0ee-4ce1-8f64-f1a572777fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+------------+-------------+\n",
      "|          name|continent|code|surface_area|geosize_group|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "|   Afghanistan|     Asia| AFG|      652090|       medium|\n",
      "|   Netherlands|   Europe| NLD|       41526|        small|\n",
      "|       Albania|   Europe| ALB|       28748|        small|\n",
      "|       Algeria|   Africa| DZA|     2381740|        large|\n",
      "|American Samoa|  Oceania| ASM|         199|        small|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sp = spark.createDataFrame(df_pd)\n",
    "df_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357c21e-d17f-4e0e-9435-334f56190ea7",
   "metadata": {},
   "source": [
    "A Spark DataFrame can be turned into a Pandas DataFrame and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f49816-3c5f-4a98-8f34-2a30629eb19b",
   "metadata": {},
   "source": [
    "## Storage\n",
    "In Spark a **DataFrame** is an immutable distributed collection of data organized by columns, and is stored in memory (or on disk if the DataFrame is very large). \n",
    "\n",
    "Similar to a SQL database, Spark uses tables on which SQL queries can be performed. A DataFrame can be turned into a **Temporary View** in order to use SQL queries on the data using. This temporary table is not yet part of the cluster, and like the DataFrame only exists in memory. \n",
    "```\n",
    "createOrReplaceTempView()\n",
    "createOrReplaceGlobalTempView()\n",
    "```\n",
    "\n",
    "The metadata of all tables in the spark session (including Temprorary Views) are stored inside of the **catalog**. A dataframe can be saved to a persistent storage system, such as Hive or Apache Parquet in order to make sure its permanently part of the Spark cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff7efa3-c6b8-451f-a655-b136e49ac0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='temp_table', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8512e3-7f22-4505-aec4-8eb5a428ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+-----+\n",
      "|           continent|count|\n",
      "+--------------------+-----+\n",
      "|              Europe|   42|\n",
      "| The Democratic R...|    1|\n",
      "|              Africa|   52|\n",
      "|             U.S.\"\"\"|    1|\n",
      "| Federated States...|    1|\n",
      "|       North America|   29|\n",
      "|          British\"\"\"|    1|\n",
      "|       South America|   12|\n",
      "|             Oceania|   18|\n",
      "|                Asia|   49|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "continent_counts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        continent,\n",
    "        COUNT(*) AS count\n",
    "    FROM temp_table\n",
    "    GROUP BY continent\n",
    "\"\"\")\n",
    "\n",
    "print(type(continent_counts))\n",
    "continent_counts.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
